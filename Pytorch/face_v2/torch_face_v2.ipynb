{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ddb672c6-24db-4403-98b1-ff995d57ad36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "面部识别神经网络设计 2.1\n",
    "\n",
    "本次实验 总共分为 三个阶段\n",
    "1，自主设计 卷积神经网络 及 全连接层 从头开始\n",
    "2，从视觉大模型中进行特征提取，进行全连接层的设计\n",
    "3，解冻视觉大模型的最后几层，进行尝试\n",
    "\n",
    "本实验使用 孪生卷积神经网络 / 孪生全连接层\n",
    "通过三元组损失函数 促使神经网络 聚拢相同样本，分散不同样本\n",
    "\n",
    "使用Pytorch 手动实现训练循环\n",
    "\n",
    "关于数据：\n",
    "使用img_align_celeba 和 106个明星面部特写 作为数据。\n",
    "其中，img_align_celeba 仅作为 三元组 的 负（不同）样本\n",
    "而，106个明星面部特写，进行数据增强（因为技术原因 暂时不使用）\n",
    "随后，每一个明星的特写 两两配对，作为三元组 的 锚定和 正（相同）样本\n",
    "\n",
    "\n",
    "实验记录 第一阶段：\n",
    "网络结构：\n",
    "    SiameseCNN(\n",
    "      (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
    "      (dens): Linear(in_features=25600, out_features=128, bias=True)\n",
    "    )\n",
    "    三元组损失函数 分割距离3， optim.Adam(model.parameters(), lr=0.0005)\n",
    "使用五个卷积层，最终加入一个全连接神经网络，帮助调整和分类\n",
    "训练集数据 拟合顺利，验证集数据 三轮训练后loss停止下降\n",
    "模型无法泛化，可能因数据太过单一 并且数据的面部角度并不完全相同。\n",
    "\n",
    "实验记录 第二阶段：\n",
    "网络 使用VGG16 神经网络的卷积神经网络作为特征提取\n",
    "但是 和阶段一相同，训练集数据拟合顺利，而验证数据集loss停止下降\n",
    "模型无法泛化，数据问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e12ef9e0-3f00-464d-b1ad-88a21a2f8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45056b7c-6ae8-4ded-81c8-cf9aa7a31a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bd701-b2fd-4a5b-9b84-4c29796d81b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b931e79-1e13-4356-9c46-7cc0506ea465",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img_106 = []\n",
    "path_img_ngt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b31ead7d-a205-45f8-8c9c-0b72428897e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"C:\\Users\\lucyc\\Desktop\\DatSet_Activate\\105_face\"\n",
    "dir_list = os.listdir(root)\n",
    "for dir_person in dir_list:\n",
    "    path_img_106.append([root+\"\\\\\"+dir_person+\"\\\\\"+x for x in os.listdir(root + \"\\\\\" + dir_person) if x[-1] == \"g\" or x[-1] == \"G\"])\n",
    "\n",
    "root = r\"C:\\Users\\lucyc\\Desktop\\DatSet_Activate\\diff_face\"\n",
    "path_img_ngt = [root+\"\\\\\"+x for x in os.listdir(root)[:50000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219d17c-5a46-42c2-b9fe-c8e8dfbd124c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc940104-ad1d-4bc1-9b4e-0a8e86935cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_trp = []\n",
    "\n",
    "#count = 0\n",
    "# for cla_imgs in path_img_106:\n",
    "#     for i in range(1, len(cla_imgs), 2):\n",
    "#         path_data_trp.append((cla_imgs[i], cla_imgs[i-1], path_img_ngt[count]))\n",
    "#         count += 1\n",
    "\n",
    "# for cla_i in range(1, len(path_img_106), 2):\n",
    "#     j = 1\n",
    "#     while j < len(path_img_106[cla_i-1]) and j//2 < len(path_img_106[cla_i]):\n",
    "#         path_data_trp.append((path_img_106[cla_i-1][j-1], path_img_106[cla_i-1][j], path_img_106[cla_i][j//2]))\n",
    "#         j += 2\n",
    "#         count += 1\n",
    "\n",
    "# for cla_i in range(2, len(path_img_106), 2):\n",
    "#     j = 1\n",
    "#     while j < len(path_img_106[cla_i-1]) and j//2 < len(path_img_106[cla_i]):\n",
    "#         path_data_trp.append((path_img_106[cla_i-1][j-1], path_img_106[cla_i-1][j], path_img_106[cla_i][j//2]))\n",
    "#         j += 2\n",
    "#         count += 1\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def random_pop(lst):\n",
    "    if lst:  # 检查列表是否非空\n",
    "        index = random.randrange(len(lst))  # 选择一个随机索引\n",
    "        return lst.pop(index)  # 弹出并返回该索引对应的元素\n",
    "    else:\n",
    "        return None  # 如果列表为空，则返回None\n",
    "\n",
    "def get_ngt(i):\n",
    "    all_path = []\n",
    "    \n",
    "    def rebuild():\n",
    "        nonlocal all_path\n",
    "        for j, x in enumerate(path_img_106):\n",
    "            if j == i:\n",
    "                continue\n",
    "            all_path += x\n",
    "            \n",
    "    rebuild()\n",
    "    while True:\n",
    "        select = random_pop(all_path)\n",
    "        if select != None:\n",
    "            yield select\n",
    "        else:\n",
    "            rebuild()\n",
    "            yield random_pop(all_path)\n",
    "\n",
    "for i in range(len(path_img_106)):\n",
    "    getngt = get_ngt(i)\n",
    "    for j in range(len(path_img_106[i])):\n",
    "        for k in range(j+1,len(path_img_106[i])):\n",
    "            path_data_trp.append([path_img_106[i][j], path_img_106[i][k], next(getngt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ae4b47e-6c69-491f-b399-ad9ef030be82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515807"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_data_trp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed9c60e8-d5be-4797-a313-eb99e09e089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path_data_trp = []\n",
    "valid_path_data_trp = []\n",
    "train_path_data_trp = []\n",
    "\n",
    "for g in path_data_trp:\n",
    "    if \"3\" in g[0] or \"3\" in g[1] or \"3\" in g[2]:\n",
    "        train_path_data_trp.append(g)\n",
    "    elif \"4\" in g[0] or \"4\" in g[1] or \"4\" in g[2]:\n",
    "        valid_path_data_trp.append(g)\n",
    "    else:\n",
    "        test_path_data_trp.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a1102-417f-4744-8d2b-f7f50568defd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "960abda1-29f8-4092-90d0-40511b101e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_path_data_trp = random.sample(test_path_data_trp, len(test_path_data_trp))\n",
    "valid_path_data_trp = random.sample(valid_path_data_trp, len(valid_path_data_trp)//50)\n",
    "train_path_data_trp = random.sample(train_path_data_trp, len(train_path_data_trp)//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1723a28a-f767-434d-9921-01bc67e70fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34891 3936 12840\n"
     ]
    }
   ],
   "source": [
    "print(len(test_path_data_trp),len(valid_path_data_trp),len(train_path_data_trp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "364ae53a-3150-4cc8-b1ce-2336e6da9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path_data_trp[0][0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9328bb7-aca5-4cf3-bf4f-7a55ea64bce4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "timg = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dde90c-41dc-4368-af78-80c0813f9e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a6f8c5b-286a-4f63-9ca0-611fe0e93f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 定义转换操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),  # 将PIL图像或NumPy ndarray转换为FloatTensor。\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 标准化，使用ImageNet的均值和标准差\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "from boring import TrpDataSet\n",
    "\n",
    "# 创建数据集\n",
    "test_dataset = TrpDataSet(test_path_data_trp, transform=transform)\n",
    "val_dataset = TrpDataSet(valid_path_data_trp, transform=transform)\n",
    "train_dataset = TrpDataSet(train_path_data_trp, transform=transform)\n",
    "# dataset = TrpDataSet(path_data_trp, transform=transform)\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.2)+1])\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [int(len(train_dataset)*0.9), int(len(train_dataset)*0.1)])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=3, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True, num_workers=3, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=3, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7022d9a2-1a1f-463b-abb3-d945add6344b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f45fcd8-eb76-4c5b-bb5c-d39fbe7fa300",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "809d8b4b-9f23-4e37-ae1a-340e3a858c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 224, 224])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf285f14-b00f-4142-a02a-90c5e7c004ea",
   "metadata": {},
   "source": [
    "to_pil_image = transforms.ToPILImage()\n",
    "pil_image = to_pil_image(a[0][127])\n",
    "# 使用PIL显示图像\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b4583-d5e0-42ea-a887-94b28da4b501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16788077-622d-481e-897d-487b3e7775e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "# 加载预训练的VGG-16模型\n",
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce7c8953-9c5d-47a5-b057-bfa8f967834e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25088"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16.classifier[0].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e09a352-8cc0-4a98-9dba-1e0ef49904e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in vgg16.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d3255a1-bc64-4a4f-828c-503605ad8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "1\n",
      "1\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "1\n",
      "1\n",
      "ReLU(inplace=True)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "1\n",
      "1\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "1\n",
      "1\n",
      "ReLU(inplace=True)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "1\n",
      "1\n",
      "ReLU(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(18,30):\n",
    "    print(vgg16.features[i])\n",
    "    #vgg16.features[i].parameter.requires_grad = True\n",
    "    for param in vgg16.features[i].parameters():\n",
    "        param.requires_grad = True\n",
    "        print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41903e15-95a6-454b-bd03-f603cf85a5d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1ace416-8a51-4d48-9b5d-62374917853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseCNN, self).__init__()\n",
    "        \n",
    "        self.last_conv_filt = 256\n",
    "        self.dens_size_sub = 10\n",
    "        self.dens_size = self.dens_size_sub*self.dens_size_sub*self.last_conv_filt\n",
    "        \n",
    "        # 定义网络层\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)  #222 -> 111\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3) #109 -> 54\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3) #52 -> 26\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3) #24 -> 12\n",
    "        self.conv5 = nn.Conv2d(128, self.last_conv_filt, 3) #10\n",
    "        # 定义全连接层\n",
    "        self.dens = nn.Linear(self.dens_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播\n",
    "        x = F.relu(self.conv1(x)) #222\n",
    "        x = F.max_pool2d(x, 2) #111\n",
    "        x = F.relu(self.conv2(x)) #109\n",
    "        x = F.max_pool2d(x, 2) #54\n",
    "        x = F.relu(self.conv3(x)) #52\n",
    "        x = F.max_pool2d(x, 2) #26\n",
    "        x = F.relu(self.conv4(x)) #24\n",
    "        x = F.max_pool2d(x, 2) #12\n",
    "        x = F.relu(self.conv5(x)) #10\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = F.relu(self.dens(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77b2ec3c-4c99-478b-971e-b828fc84ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCNN_VGG16(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseCNN_VGG16, self).__init__()\n",
    "        # 定义网络层\n",
    "        self.vgg16 = model = nn.Sequential(\n",
    "                            vgg16.features,\n",
    "                            vgg16.avgpool\n",
    "        )\n",
    "        self.feature = vgg16.classifier[0].in_features\n",
    "        # 定义全连接层\n",
    "        # self.dens = nn.Linear(self.feature, 1024)\n",
    "        # self.drop = nn.Dropout(p=0.5, inplace=False)\n",
    "        # self.dens2 = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播\n",
    "        #x = x.flatten(start_dim=1)\n",
    "        x = self.vgg16(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # x = F.relu(self.dens(x))\n",
    "        # x = self.drop(x)\n",
    "        # x = F.relu(self.dens2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717066e-aafc-4067-8f50-241c4e3d2f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d27f441d-8ad3-4739-abd0-9a0f4ca744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TripLoss, self).__init__()\n",
    "        # 这里可以初始化任何需要的参数或层\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # 自定义损失的计算逻辑\n",
    "        # 例如，我们可以计算平均绝对误差     \n",
    "        # 计算锚点与正负样本之间的距离\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # 对每个元素平方后求和\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        \n",
    "        # 计算三元组损失\n",
    "        losses = F.relu(distance_positive - distance_negative + 3)\n",
    "        \n",
    "        # 返回平均损失\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf56f3b8-cde7-400e-a11e-f10619b7952a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiameseCNN_VGG16(\n",
      "  (vgg16): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  )\n",
      ")\n",
      "Epoch [1/30], Loss: 43.50, Val_loss: 3.02\n",
      "Epoch [2/30], Loss: 2.38, Val_loss: 2.31\n",
      "Epoch [3/30], Loss: 1.87, Val_loss: 2.02\n",
      "Epoch [4/30], Loss: 1.56, Val_loss: 1.93\n",
      "Epoch [5/30], Loss: 1.35, Val_loss: 1.90\n",
      "Epoch [6/30], Loss: 1.15, Val_loss: 1.86\n",
      "Epoch [7/30], Loss: 0.98, Val_loss: 1.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(anchor, positive, negative)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 反向传播计算当前的梯度\u001b[39;00m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 更新参数\u001b[39;00m\n\u001b[0;32m     38\u001b[0m loss_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# 构建模型\n",
    "#model = SiameseCNN()\n",
    "model = SiameseCNN_VGG16()\n",
    "print(model)\n",
    "\n",
    "model.to(device)  # 将模型发送到GPU，如果有的话\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = TripLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "\n",
    "    loss_acc = 0.\n",
    "    train_num = 0\n",
    "    for inputs in train_loader:\n",
    "        anchor = inputs[0].to(device)\n",
    "        positive = inputs[1].to(device)\n",
    "        negative = inputs[2].to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        anchor = model(anchor)\n",
    "        positive = model(positive)\n",
    "        negative = model(negative)\n",
    "\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        loss = criterion(anchor, positive, negative)\n",
    "        # 反向传播和优化\n",
    "        loss.backward()  # 反向传播计算当前的梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        loss_acc += loss.item()\n",
    "        train_num += 1\n",
    "\n",
    "        #print(loss)\n",
    "\n",
    "    model.eval() \n",
    "    val_loss_acc = 0\n",
    "    val_num = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs in val_loader:\n",
    "            anchor = inputs[0].to(device)\n",
    "            positive = inputs[1].to(device)\n",
    "            negative = inputs[2].to(device)\n",
    "            \n",
    "            anchor = model(anchor)\n",
    "            positive = model(positive)\n",
    "            negative = model(negative)\n",
    "\n",
    "            loss = criterion(anchor, positive, negative)\n",
    "            #loss += abs(5-criterion(anchor, negative))\n",
    "            val_loss_acc += loss\n",
    "            val_num += 1\n",
    "\n",
    "    print(\"Epoch [{}/{}], Loss: {:.2f}, Val_loss: {:.2f}\".format(epoch+1, num_epochs, loss_acc/train_num, val_loss_acc/val_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728c775-ec35-45d3-8db6-adb4c69086f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89feb00c-6a48-44cc-811d-c02d5d54d1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "67618286-e859-44a2-b710-f2de28ad6c2e",
   "metadata": {},
   "source": [
    "# 假设 model 是你的模型，val_loader 是你的验证数据加载器\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():  # 在此模式下，不计算梯度，节省计算资源\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)  # 将数据和标签移动到正确的设备\n",
    "        outputs = model(data)\n",
    "        #outputs = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # 获取预测结果\n",
    "        total_samples += target.size(0)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f'Accuracy of the model on the validation data: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "101e3b06-58b8-4006-b174-287e9414715b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap: 15.09, an: 17.17\n",
      "ap: 15.72, an: 18.54\n",
      "ap: 11.44, an: 13.71\n",
      "ap: 18.71, an: 19.48\n",
      "ap: 13.59, an: 12.78\n",
      "ap: 14.27, an: 33.61\n",
      "ap: 21.10, an: 14.08\n",
      "ap: 14.05, an: 17.78\n",
      "ap: 13.60, an: 14.35\n",
      "ap: 14.66, an: 33.29\n",
      "ap: 18.39, an: 26.68\n",
      "ap: 10.76, an: 11.51\n",
      "ap: 22.92, an: 41.89\n",
      "ap: 14.90, an: 35.70\n",
      "ap: 18.19, an: 23.12\n",
      "ap: 12.35, an: 18.88\n",
      "ap: 14.61, an: 10.00\n",
      "ap: 16.61, an: 30.87\n",
      "ap: 11.70, an: 15.12\n",
      "ap: 13.23, an: 16.20\n",
      "ap: 11.60, an: 11.13\n",
      "ap: 9.49, an: 14.11\n",
      "ap: 20.66, an: 19.79\n",
      "ap: 16.58, an: 32.67\n",
      "ap: 16.40, an: 28.39\n",
      "ap: 15.24, an: 17.84\n",
      "ap: 19.44, an: 21.71\n",
      "ap: 16.95, an: 27.66\n",
      "ap: 15.96, an: 24.82\n",
      "ap: 16.21, an: 26.78\n",
      "ap: 12.69, an: 9.67\n",
      "ap: 15.59, an: 27.69\n",
      "ap: 15.97, an: 16.59\n",
      "ap: 16.03, an: 12.51\n",
      "ap: 16.29, an: 12.24\n",
      "ap: 22.12, an: 26.07\n",
      "ap: 16.78, an: 23.91\n",
      "ap: 18.07, an: 16.85\n",
      "ap: 18.17, an: 23.22\n",
      "ap: 17.08, an: 43.02\n",
      "ap: 16.84, an: 21.75\n",
      "ap: 16.53, an: 27.05\n",
      "ap: 16.60, an: 20.33\n",
      "ap: 15.50, an: 17.74\n",
      "ap: 14.21, an: 18.84\n",
      "ap: 20.44, an: 14.85\n",
      "ap: 11.23, an: 29.39\n",
      "ap: 19.90, an: 26.97\n",
      "ap: 18.65, an: 15.55\n",
      "ap: 23.85, an: 27.03\n",
      "ap: 17.84, an: 17.56\n",
      "ap: 18.97, an: 25.29\n",
      "ap: 14.85, an: 20.08\n",
      "ap: 14.45, an: 23.15\n",
      "ap: 16.21, an: 17.04\n",
      "ap: 15.71, an: 19.67\n",
      "ap: 20.22, an: 15.22\n",
      "ap: 17.70, an: 15.08\n",
      "ap: 15.88, an: 17.98\n",
      "ap: 20.99, an: 17.72\n",
      "ap: 13.45, an: 16.67\n",
      "ap: 21.11, an: 25.57\n",
      "ap: 17.70, an: 18.48\n",
      "ap: 18.01, an: 23.03\n",
      "ap: 18.06, an: 29.29\n",
      "ap: 17.46, an: 16.09\n",
      "ap: 11.94, an: 13.97\n",
      "ap: 12.48, an: 18.54\n",
      "ap: 12.82, an: 22.92\n",
      "ap: 11.31, an: 14.12\n",
      "ap: 25.36, an: 21.47\n",
      "ap: 12.37, an: 10.51\n",
      "ap: 13.40, an: 15.26\n",
      "ap: 16.94, an: 17.75\n",
      "ap: 16.47, an: 20.05\n",
      "ap: 14.33, an: 26.77\n",
      "ap: 24.31, an: 16.76\n",
      "ap: 10.65, an: 19.89\n",
      "ap: 19.24, an: 20.92\n",
      "ap: 18.24, an: 20.84\n",
      "ap: 9.40, an: 14.14\n",
      "ap: 14.52, an: 16.69\n",
      "ap: 20.91, an: 18.20\n",
      "ap: 16.11, an: 31.00\n",
      "ap: 13.43, an: 21.49\n",
      "ap: 15.51, an: 11.80\n",
      "ap: 11.33, an: 26.49\n",
      "ap: 20.44, an: 22.44\n",
      "ap: 20.62, an: 30.86\n",
      "ap: 20.02, an: 20.30\n",
      "ap: 14.02, an: 22.68\n",
      "ap: 16.88, an: 27.40\n",
      "ap: 20.01, an: 20.02\n",
      "ap: 13.55, an: 32.23\n",
      "ap: 10.74, an: 13.39\n",
      "ap: 15.04, an: 15.81\n",
      "ap: 10.37, an: 18.63\n",
      "ap: 16.45, an: 18.04\n",
      "ap: 18.19, an: 19.23\n",
      "ap: 10.21, an: 10.77\n",
      "ap: 13.88, an: 21.94\n",
      "ap: 13.91, an: 23.17\n",
      "ap: 15.04, an: 20.52\n",
      "ap: 19.56, an: 15.35\n",
      "ap: 17.68, an: 20.00\n",
      "ap: 18.28, an: 20.22\n",
      "ap: 18.98, an: 28.64\n",
      "ap: 14.22, an: 27.52\n",
      "ap: 16.57, an: 13.12\n",
      "ap: 17.78, an: 16.37\n",
      "ap: 12.25, an: 13.91\n",
      "ap: 13.97, an: 12.45\n",
      "ap: 19.07, an: 17.17\n",
      "ap: 12.51, an: 13.30\n",
      "ap: 16.30, an: 20.36\n",
      "ap: 110.96, an: 19.15\n",
      "ap: 12.54, an: 38.80\n",
      "ap: 21.39, an: 37.49\n",
      "ap: 18.18, an: 15.22\n",
      "ap: 13.63, an: 17.74\n",
      "ap: 14.63, an: 12.14\n",
      "ap: 16.32, an: 22.04\n",
      "ap: 15.44, an: 20.40\n",
      "ap: 14.88, an: 20.21\n",
      "ap: 14.75, an: 13.52\n",
      "ap: 20.67, an: 21.34\n",
      "ap: 15.71, an: 27.42\n",
      "ap: 17.12, an: 15.40\n",
      "ap: 15.39, an: 26.07\n",
      "ap: 15.10, an: 13.74\n",
      "ap: 20.19, an: 17.33\n",
      "ap: 16.07, an: 16.13\n",
      "ap: 9.90, an: 27.35\n",
      "ap: 10.83, an: 10.44\n",
      "ap: 16.61, an: 26.30\n",
      "ap: 13.65, an: 9.04\n",
      "ap: 16.21, an: 31.71\n",
      "ap: 19.06, an: 13.91\n",
      "ap: 18.65, an: 24.11\n",
      "ap: 17.87, an: 16.07\n",
      "ap: 21.04, an: 11.14\n",
      "ap: 22.86, an: 36.78\n",
      "ap: 19.17, an: 35.40\n",
      "ap: 22.70, an: 22.10\n",
      "ap: 20.30, an: 27.54\n",
      "ap: 21.34, an: 14.63\n",
      "ap: 21.44, an: 25.21\n",
      "ap: 11.90, an: 32.01\n",
      "ap: 10.76, an: 8.95\n",
      "ap: 15.29, an: 33.18\n",
      "ap: 17.90, an: 25.32\n",
      "ap: 18.39, an: 18.04\n",
      "ap: 15.79, an: 17.70\n",
      "ap: 14.81, an: 14.51\n",
      "ap: 16.42, an: 14.88\n",
      "ap: 13.89, an: 19.90\n",
      "ap: 13.56, an: 18.10\n",
      "ap: 11.57, an: 14.98\n",
      "ap: 9.54, an: 16.87\n",
      "ap: 11.84, an: 11.64\n",
      "ap: 15.14, an: 12.62\n",
      "ap: 14.09, an: 15.49\n",
      "ap: 14.68, an: 18.40\n",
      "ap: 13.32, an: 19.21\n",
      "ap: 12.05, an: 28.57\n",
      "ap: 20.89, an: 20.35\n",
      "ap: 15.65, an: 20.02\n",
      "ap: 16.74, an: 30.29\n",
      "ap: 10.26, an: 11.10\n",
      "ap: 16.05, an: 12.18\n",
      "ap: 14.98, an: 27.55\n",
      "ap: 11.77, an: 12.98\n",
      "ap: 14.24, an: 13.21\n",
      "ap: 15.02, an: 16.97\n",
      "ap: 11.07, an: 17.01\n",
      "ap: 15.55, an: 12.18\n",
      "ap: 10.60, an: 30.69\n",
      "ap: 21.40, an: 28.06\n",
      "ap: 24.51, an: 26.19\n",
      "ap: 24.86, an: 27.49\n",
      "ap: 14.83, an: 16.55\n",
      "ap: 16.89, an: 33.38\n",
      "ap: 17.28, an: 30.96\n",
      "ap: 13.93, an: 27.88\n",
      "ap: 18.41, an: 21.16\n",
      "ap: 15.90, an: 11.26\n",
      "ap: 16.35, an: 14.32\n",
      "ap: 12.79, an: 15.89\n",
      "ap: 11.38, an: 13.30\n",
      "ap: 18.05, an: 21.07\n",
      "ap: 20.54, an: 26.96\n",
      "ap: 14.86, an: 17.26\n",
      "ap: 9.27, an: 18.41\n",
      "ap: 11.51, an: 17.90\n",
      "ap: 20.53, an: 20.58\n",
      "ap: 17.26, an: 15.16\n",
      "ap: 13.61, an: 23.44\n",
      "ap: 13.18, an: 19.73\n",
      "ap: 17.65, an: 25.33\n",
      "ap: 15.57, an: 22.05\n",
      "ap: 15.33, an: 31.42\n",
      "ap: 19.34, an: 13.88\n",
      "ap: 18.01, an: 18.16\n",
      "ap: 19.64, an: 23.26\n",
      "ap: 16.72, an: 15.83\n",
      "ap: 22.10, an: 16.88\n",
      "ap: 11.65, an: 20.45\n",
      "ap: 17.65, an: 23.96\n",
      "ap: 19.46, an: 20.78\n",
      "ap: 12.08, an: 22.84\n",
      "ap: 13.76, an: 15.63\n",
      "ap: 17.05, an: 22.77\n",
      "ap: 14.94, an: 16.00\n",
      "ap: 9.86, an: 12.60\n",
      "ap: 10.51, an: 14.02\n",
      "ap: 20.72, an: 19.66\n",
      "ap: 13.02, an: 28.96\n",
      "ap: 9.69, an: 22.71\n",
      "ap: 10.02, an: 17.14\n",
      "ap: 13.12, an: 19.54\n",
      "ap: 12.53, an: 20.18\n",
      "ap: 15.80, an: 24.60\n",
      "ap: 14.73, an: 18.88\n",
      "ap: 13.59, an: 23.62\n",
      "ap: 19.00, an: 17.88\n",
      "ap: 16.44, an: 19.56\n",
      "ap: 11.49, an: 27.01\n",
      "ap: 11.87, an: 12.62\n",
      "ap: 18.90, an: 30.19\n",
      "ap: 8.96, an: 8.85\n",
      "ap: 12.43, an: 17.21\n",
      "ap: 22.23, an: 24.40\n",
      "ap: 15.10, an: 13.84\n",
      "ap: 23.59, an: 18.34\n",
      "ap: 18.29, an: 24.07\n",
      "ap: 17.50, an: 20.68\n",
      "ap: 13.59, an: 13.92\n",
      "ap: 18.13, an: 15.72\n",
      "ap: 15.20, an: 28.79\n",
      "ap: 12.31, an: 15.71\n",
      "ap: 18.11, an: 18.62\n",
      "ap: 15.88, an: 15.32\n",
      "ap: 10.77, an: 15.81\n",
      "ap: 18.44, an: 21.34\n",
      "ap: 9.17, an: 28.82\n",
      "ap: 13.98, an: 21.41\n",
      "ap: 19.16, an: 23.25\n",
      "ap: 16.28, an: 15.60\n",
      "ap: 16.03, an: 16.16\n",
      "ap: 22.02, an: 17.67\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "def loss_cal(v1, v2):\n",
    "    apd = torch.sum((v1 - v2) ** 2, dim=1)\n",
    "    return apd.mean() # 平均欧几里得距离\n",
    "\n",
    "model.eval()\n",
    "tl = iter(test_loader)\n",
    "pass_num = 0\n",
    "fail_num = 0\n",
    "with torch.no_grad():\n",
    "    for _ in range(250):\n",
    "        anchor, positive, negative = next(tl)\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        a = model(anchor)\n",
    "        p = model(positive)\n",
    "        n = model(negative)\n",
    "        \n",
    "        if loss_cal(a,p) < loss_cal(a,n):\n",
    "            pass_num += 1\n",
    "        else:\n",
    "            fail_num += 1\n",
    "            \n",
    "        print(\"ap: {:.2f}, an: {:.2f}\".format(loss_cal(a,p),loss_cal(a,n)))\n",
    "\n",
    "print(pass_num)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be304637-b313-4d15-bb8e-3c92d65b3388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4359e95-5e7e-4a76-ae7e-eb350fd06cfc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16.0.0.weight\n",
      "vgg16.0.0.bias\n",
      "vgg16.0.2.weight\n",
      "vgg16.0.2.bias\n",
      "vgg16.0.5.weight\n",
      "vgg16.0.5.bias\n",
      "vgg16.0.7.weight\n",
      "vgg16.0.7.bias\n",
      "vgg16.0.10.weight\n",
      "vgg16.0.10.bias\n",
      "vgg16.0.12.weight\n",
      "vgg16.0.12.bias\n",
      "vgg16.0.14.weight\n",
      "vgg16.0.14.bias\n",
      "vgg16.0.17.weight\n",
      "vgg16.0.17.bias\n",
      "vgg16.0.19.weight\n",
      "vgg16.0.19.bias\n",
      "vgg16.0.21.weight\n",
      "vgg16.0.21.bias\n",
      "vgg16.0.24.weight\n",
      "vgg16.0.24.bias\n",
      "vgg16.0.26.weight\n",
      "vgg16.0.26.bias\n",
      "vgg16.0.28.weight\n",
      "vgg16.0.28.bias\n",
      "dens.weight\n",
      "dens.bias\n",
      "dens2.weight\n",
      "dens2.bias\n"
     ]
    }
   ],
   "source": [
    "for a, b in model.named_parameters():\n",
    "    print(a)\n",
    "    b.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7644bb57-b41c-425f-9fb7-3c2733187f7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "True\n",
      "conv1.bias\n",
      "True\n",
      "conv2.weight\n",
      "True\n",
      "conv2.bias\n",
      "True\n",
      "conv3.weight\n",
      "True\n",
      "conv3.bias\n",
      "True\n",
      "conv4.weight\n",
      "True\n",
      "conv4.bias\n",
      "True\n",
      "conv5.weight\n",
      "True\n",
      "conv5.bias\n",
      "True\n",
      "dens.weight\n",
      "True\n",
      "dens.bias\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for a, b in model.named_parameters():\n",
    "    print(a)\n",
    "    print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06f49d-c33a-40bf-8695-df6e2e679a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa6e21e7-bd6d-45ce-b8e6-eb17ae5b24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'face_v2_full.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3918e487-4985-4d84-a259-e9d7f890d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'face_v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45872a2-00c7-4b6c-9250-d5a9883ad6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf1fc2-4921-40b5-9007-b7c577602dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d86dea-df87-4627-93fc-9465c62a0c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aigo",
   "language": "python",
   "name": "aigo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
