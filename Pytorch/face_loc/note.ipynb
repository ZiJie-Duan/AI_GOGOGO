{
 "cells": [
  {
   "cell_type": "raw",
   "id": "31b8b2e5-2f57-404d-9acf-f23c0726c30a",
   "metadata": {},
   "source": [
    "人脸检测神经网络设计 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc06e77-3983-4f33-9050-d5edbb55bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8204a968-0a5f-4d60-b814-e3c48a0e7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20cbc92-3077-4f1a-98bc-d18e7d9ef9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame import CELEBADriver, WFDriver, MTCDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81167968-7b8a-4c8c-9af7-f631e7dfd845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d041d96-2987-4db3-8ce5-2fdb94960d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义转换操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(12),\n",
    "    transforms.CenterCrop(12),\n",
    "    transforms.ToTensor(),  # 将PIL图像或NumPy ndarray转换为FloatTensor。\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 标准化，使用ImageNet的均值和标准差\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce958c5d-85fa-48b9-92aa-c9a3c73587fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bbox_path = r\"C:\\Users\\lucyc\\Desktop\\celebA\\list_bbox_celeba.csv\"\n",
    "ldmk_path = r\"C:\\Users\\lucyc\\Desktop\\celebA\\list_landmarks_align_celeba.csv\"\n",
    "basic_path = r\"C:\\Users\\lucyc\\Desktop\\celebA\\img_align_celeba\\img_align_celeba\"\n",
    "\n",
    "cead = CELEBADriver(bbox_path, ldmk_path, basic_path)\n",
    "\n",
    "mat_path = r\"C:\\Users\\lucyc\\Desktop\\faces\\WIDER_train\\WIDER_train\\images\"\n",
    "clas_root_path = r\"C:\\Users\\lucyc\\Desktop\\faces\\wider_face_split\\wider_face_split\\wider_face_train.mat\"\n",
    "\n",
    "wfd = WFDriver(clas_root_path, mat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5cf1dfe-a3b2-40ff-9291-e2d45380b2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "!!! SERIOUS RANDOM INIT DATASET ALARM !!! type ‘y’ to continue...  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SET\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "!!! SERIOUS RANDOM INIT DATASET ALARM !!! type ‘y’ to continue...  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SET\n"
     ]
    }
   ],
   "source": [
    "cead.random_init()\n",
    "wfd.random_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f810cd66-b3f7-4cdf-96d3-94bcc973353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12880"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wfd.dataset_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b28f3e1-26d0-4645-9efe-ded1e970455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "test_dataset = MTCDataSet(wfd, cead, [10304,12870], transform)\n",
    "val_dataset = MTCDataSet(wfd, cead, [7728,10304], transform)\n",
    "train_dataset = MTCDataSet(wfd, cead, [0,7728], transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656ac5ce-dd67-4d84-9c77-3e913eacf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=3, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=True, num_workers=3, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e3b97-531f-432b-943f-05799c0b8939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6bf8f41-f4ef-4d1a-9eb2-7f0232f8e35f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\lucyc\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\lucyc\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\lucyc\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\lucyc\\Desktop\\AI_GOGOGO\\Pytorch\\face_loc\\frame.py\", line 167, in __getitem__\n    for img, bbox in self.wfd_random_cut(wfd_img, wfd_bbox, type_of_sample):\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\lucyc\\Desktop\\AI_GOGOGO\\Pytorch\\face_loc\\frame.py\", line 296, in wfd_random_cut\n    assert imgs_bboxs != [(None, None)], \"imgs_bboxs is empty {}\".format(imgs_bboxs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\lucyc\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\lucyc\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\lucyc\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\lucyc\\Desktop\\AI_GOGOGO\\Pytorch\\face_loc\\frame.py\", line 167, in __getitem__\n    for img, bbox in self.wfd_random_cut(wfd_img, wfd_bbox, type_of_sample):\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\lucyc\\Desktop\\AI_GOGOGO\\Pytorch\\face_loc\\frame.py\", line 296, in wfd_random_cut\n    assert imgs_bboxs != [(None, None)], \"imgs_bboxs is empty {}\".format(imgs_bboxs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    }
   ],
   "source": [
    "a = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9d8ed-17d4-4916-98c1-3a18e3f6102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405dc44-452c-4940-8867-f461e137ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c0246-0af8-4ea0-b9dc-a6dffd46bdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203feae-3bb2-4f22-9809-dfa1bed90b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80c66a-6a07-483e-bdec-a80686fc1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"CPU\")\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c49d2-e065-4508-8c1d-64e4d5da4754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b65901-3e1b-4723-b40a-3ad7f1753954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306c4ac-24ba-43a4-a46c-e60ac3615d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2d4f2-edd5-42f8-a50b-567ef8fa9506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e9758-d831-449b-a7e8-d16d0a7401a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bcd23-51f9-4dd3-a6fd-00a0f1cd4e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed60163-fe48-4580-af24-e666a0760a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fd2f3-70ae-4f83-a85b-616c48a64ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e1cf592-41d0-416d-ab62-529c3bc747a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mat = scipy.io.loadmat(r\"C:\\Users\\lucyc\\Desktop\\faces\\wider_face_split\\wider_face_split\\wider_face_train.mat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea1662a0-70ac-4ec6-b648-1b5cd59f07af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'blur_label_list', 'event_list', 'expression_label_list', 'face_bbx_list', 'file_list', 'illumination_label_list', 'invalid_label_list', 'occlusion_label_list', 'pose_label_list'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0d587cf-7f0d-469e-9322-3c4603a2e3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[\"face_bbx_list\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "93dad397-344b-4b19-8815-f422520fbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfd = WFDriver(r\"C:\\Users\\lucyc\\Desktop\\faces\\wider_face_split\\wider_face_split\\wider_face_train.mat\",\n",
    "               r\"C:\\Users\\lucyc\\Desktop\\faces\\WIDER_train\\WIDER_train\\images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "beb0d373-932e-4c81-af9b-dadfa1c90989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lucyc\\\\Desktop\\\\faces\\\\WIDER_train\\\\WIDER_train\\\\images\\\\61--Street_Battle\\\\61_Street_Battle_streetfight_61_382.jpg'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfd.get_file_path(61,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ec696b49-383e-4c0a-be84-fe8d70b0118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[223, 289,  32,  44],\n",
       "       [357, 258,  20,  27],\n",
       "       [411, 313,  18,  26],\n",
       "       [346, 224,  11,  15],\n",
       "       [298, 224,  15,  20],\n",
       "       [193, 244,  15,  14],\n",
       "       [223, 232,  12,  14],\n",
       "       [135, 239,  26,  35],\n",
       "       [ 86, 222,   7,   8],\n",
       "       [100, 231,   6,   8],\n",
       "       [176, 232,   8,  10],\n",
       "       [389, 238,  10,  14],\n",
       "       [474, 212,  12,  13],\n",
       "       [ 16, 225,   9,  11]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7cb2d-0b76-435c-bc1a-e564fe988ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
