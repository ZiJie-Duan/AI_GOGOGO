{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5fe8a-f62e-4564-af73-7bd1c64a13fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b58529de-63dc-40a0-a1fd-4f68ba717da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "f0fa7c1f-a13b-4a14-8161-11076baa79bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "circle_data = []\n",
    "trig_data = []\n",
    "\n",
    "def clean_data(data):\n",
    "    ndata = []\n",
    "    data_loss = 0\n",
    "    for e in data:\n",
    "        if e == \"\":\n",
    "            data_loss +=1\n",
    "            continue\n",
    "        ndata.append(int(float(e)))\n",
    "    for _ in range(data_loss):\n",
    "        ndata.append(0)\n",
    "    return ndata\n",
    "\n",
    "# 打开CSV文件\n",
    "with open('0.csv', mode='r', encoding='utf-8') as file:\n",
    "    # 创建CSV阅读器\n",
    "    reader = csv.reader(file)\n",
    "    # 逐行读取CSV文件\n",
    "    for row in reader:\n",
    "        row = row[:-1]\n",
    "        row = clean_data(row)\n",
    "        circle_data.append(row)\n",
    "\n",
    "# 打开CSV文件\n",
    "with open('1.csv', mode='r', encoding='utf-8') as file:\n",
    "    # 创建CSV阅读器\n",
    "    reader = csv.reader(file)\n",
    "    # 逐行读取CSV文件\n",
    "    for row in reader:\n",
    "        row = row[:-1]\n",
    "        row = clean_data(row)\n",
    "        trig_data.append(row)\n",
    "\n",
    "t_c_x = circle_data[:-20]\n",
    "v_c_x = circle_data[-20:]\n",
    "t_t_x = trig_data[:-20]\n",
    "v_t_x = trig_data[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46cb50-741e-4c94-9b14-2b6208259fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "af2fe79b-e72c-46f0-86c6-1077be8f0b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 120)\n",
      "(115, 120)\n",
      "tf.Tensor(\n",
      "[[-3.7986636e-03 -3.7986636e-03 -3.7986636e-03 ... -3.7986636e-03\n",
      "  -3.7986636e-03 -3.7986636e-03]\n",
      " [ 2.3285234e-01 -2.3239285e-01 -8.8518214e-01 ... -5.9387922e-01\n",
      "   8.7185621e-02 -7.0067090e-01]\n",
      " [ 2.2770584e-01 -1.5318394e-04 -9.9555802e-01 ... -4.7973531e-01\n",
      "   7.2450399e-02 -6.1342400e-01]\n",
      " ...\n",
      " [ 6.0349822e-02 -3.5444021e-02 -5.6367397e-02 ...  3.2987165e-01\n",
      "  -2.5242805e-02  1.6193366e-01]\n",
      " [-3.7986636e-03 -3.7986636e-03 -3.7986636e-03 ... -3.7986636e-03\n",
      "  -3.7986636e-03 -3.7986636e-03]\n",
      " [-3.7986636e-03 -3.7986636e-03 -3.7986636e-03 ... -3.7986636e-03\n",
      "  -3.7986636e-03 -3.7986636e-03]], shape=(216, 120), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c_d = tf.Variable(initial_value=t_c_x, dtype=tf.float32)\n",
    "t_d = tf.Variable(initial_value=t_t_x, dtype=tf.float32)\n",
    "train_data = tf.concat([c_d,t_d], axis = 0)\n",
    "\n",
    "labble1 = tf.zeros(shape=(101, 1), dtype=tf.float32)\n",
    "labble2 = tf.ones(shape=(115, 1), dtype=tf.float32)\n",
    "labble = tf.concat([labble1,labble2], axis = 0)\n",
    "\n",
    "# 计算最大值和最小值\n",
    "data_min = tf.reduce_min(train_data)\n",
    "data_max = tf.reduce_max(train_data)\n",
    "\n",
    "# 应用最小-最大缩放公式\n",
    "scaled_data = 2 * ((train_data - data_min) / (data_max - data_min)) - 1\n",
    "\n",
    "# 检查结果\n",
    "print(c_d.shape) # (101, 120)\n",
    "print(t_d.shape) # (115, 120)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8e0db9ab-df2f-4310-a9e1-f51767a69f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(initial_value = tf.random.uniform(shape = (120, 50)))\n",
    "b1 = tf.Variable(initial_value = tf.zeros(shape=(50,)))\n",
    "W2 = tf.Variable(initial_value = tf.random.uniform(shape = (50, 18)))\n",
    "b2 = tf.Variable(initial_value = tf.zeros(shape=(18,)))\n",
    "W3 = tf.Variable(initial_value = tf.random.uniform(shape = (18, 1)))\n",
    "b3 = tf.Variable(initial_value = tf.zeros(shape=(1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "677c959a-5da1-479a-9ebb-8de875626f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def model(inputs):\n",
    "    l1 = tf.matmul(inputs, W1) + b1\n",
    "    l2 = tf.matmul(tf.nn.relu(l1), W2) + b2\n",
    "    l3 = tf.matmul(tf.nn.relu(l2), W3) + b3\n",
    "    return tf.sigmoid(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "629fbe90-72cf-4488-95d4-f201d2d61fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数 为 方差\n",
    "def square_loss(targets, predictions):\n",
    "    per_sample_losses = tf.square(targets - predictions)\n",
    "    return  tf.reduce_mean(per_sample_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "b515dbce-97a7-4235-8e8f-257a385674bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "la = 0.3\n",
    "\n",
    "def training_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = model(inputs)\n",
    "        loss = square_loss(targets, y)\n",
    "    W1l,b1l,W2l,b2l,W3l,b3l = tape.gradient(loss, [W1,b1,W2,b2,W3,b3])\n",
    "    W1.assign_sub(W1l*la)\n",
    "    W2.assign_sub(W2l*la)\n",
    "    W3.assign_sub(W3l*la)\n",
    "    b1.assign_sub(b1l*la)\n",
    "    b2.assign_sub(b2l*la)\n",
    "    b3.assign_sub(b3l*la)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "3b8752dd-ab66-4aef-a3b9-1deee88c70c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 0.4988\n",
      "Loss at step 1: 0.4982\n",
      "Loss at step 2: 0.4975\n",
      "Loss at step 3: 0.4969\n",
      "Loss at step 4: 0.4962\n",
      "Loss at step 5: 0.4956\n",
      "Loss at step 6: 0.4950\n",
      "Loss at step 7: 0.4944\n",
      "Loss at step 8: 0.4938\n",
      "Loss at step 9: 0.4932\n",
      "Loss at step 10: 0.4927\n",
      "Loss at step 11: 0.4921\n",
      "Loss at step 12: 0.4916\n",
      "Loss at step 13: 0.4911\n",
      "Loss at step 14: 0.4906\n",
      "Loss at step 15: 0.4900\n",
      "Loss at step 16: 0.4895\n",
      "Loss at step 17: 0.4891\n",
      "Loss at step 18: 0.4886\n",
      "Loss at step 19: 0.4881\n",
      "Loss at step 20: 0.4877\n",
      "Loss at step 21: 0.4872\n",
      "Loss at step 22: 0.4868\n",
      "Loss at step 23: 0.4864\n",
      "Loss at step 24: 0.4860\n",
      "Loss at step 25: 0.4855\n",
      "Loss at step 26: 0.4851\n",
      "Loss at step 27: 0.4848\n",
      "Loss at step 28: 0.4844\n",
      "Loss at step 29: 0.4840\n",
      "Loss at step 30: 0.4836\n",
      "Loss at step 31: 0.4833\n",
      "Loss at step 32: 0.4829\n",
      "Loss at step 33: 0.4826\n",
      "Loss at step 34: 0.4822\n",
      "Loss at step 35: 0.4819\n",
      "Loss at step 36: 0.4816\n",
      "Loss at step 37: 0.4813\n",
      "Loss at step 38: 0.4810\n",
      "Loss at step 39: 0.4807\n",
      "Loss at step 40: 0.4804\n",
      "Loss at step 41: 0.4801\n",
      "Loss at step 42: 0.4798\n",
      "Loss at step 43: 0.4795\n",
      "Loss at step 44: 0.4793\n",
      "Loss at step 45: 0.4790\n",
      "Loss at step 46: 0.4787\n",
      "Loss at step 47: 0.4785\n",
      "Loss at step 48: 0.4782\n",
      "Loss at step 49: 0.4780\n",
      "Loss at step 50: 0.4778\n",
      "Loss at step 51: 0.4775\n",
      "Loss at step 52: 0.4773\n",
      "Loss at step 53: 0.4771\n",
      "Loss at step 54: 0.4769\n",
      "Loss at step 55: 0.4766\n",
      "Loss at step 56: 0.4764\n",
      "Loss at step 57: 0.4762\n",
      "Loss at step 58: 0.4760\n",
      "Loss at step 59: 0.4758\n",
      "Loss at step 60: 0.4756\n",
      "Loss at step 61: 0.4754\n",
      "Loss at step 62: 0.4753\n",
      "Loss at step 63: 0.4751\n",
      "Loss at step 64: 0.4749\n",
      "Loss at step 65: 0.4747\n",
      "Loss at step 66: 0.4746\n",
      "Loss at step 67: 0.4744\n",
      "Loss at step 68: 0.4742\n",
      "Loss at step 69: 0.4741\n",
      "Loss at step 70: 0.4739\n",
      "Loss at step 71: 0.4737\n",
      "Loss at step 72: 0.4736\n",
      "Loss at step 73: 0.4734\n",
      "Loss at step 74: 0.4733\n",
      "Loss at step 75: 0.4732\n",
      "Loss at step 76: 0.4730\n",
      "Loss at step 77: 0.4729\n",
      "Loss at step 78: 0.4727\n",
      "Loss at step 79: 0.4726\n",
      "Loss at step 80: 0.4725\n",
      "Loss at step 81: 0.4724\n",
      "Loss at step 82: 0.4722\n",
      "Loss at step 83: 0.4721\n",
      "Loss at step 84: 0.4720\n",
      "Loss at step 85: 0.4719\n",
      "Loss at step 86: 0.4717\n",
      "Loss at step 87: 0.4716\n",
      "Loss at step 88: 0.4715\n",
      "Loss at step 89: 0.4714\n",
      "Loss at step 90: 0.4713\n",
      "Loss at step 91: 0.4712\n",
      "Loss at step 92: 0.4711\n",
      "Loss at step 93: 0.4710\n",
      "Loss at step 94: 0.4709\n",
      "Loss at step 95: 0.4708\n",
      "Loss at step 96: 0.4707\n",
      "Loss at step 97: 0.4706\n",
      "Loss at step 98: 0.4705\n",
      "Loss at step 99: 0.4704\n",
      "Loss at step 100: 0.4703\n",
      "Loss at step 101: 0.4702\n",
      "Loss at step 102: 0.4701\n",
      "Loss at step 103: 0.4701\n",
      "Loss at step 104: 0.4700\n",
      "Loss at step 105: 0.4699\n",
      "Loss at step 106: 0.4698\n",
      "Loss at step 107: 0.4697\n",
      "Loss at step 108: 0.4697\n",
      "Loss at step 109: 0.4696\n",
      "Loss at step 110: 0.4695\n",
      "Loss at step 111: 0.4694\n",
      "Loss at step 112: 0.4694\n",
      "Loss at step 113: 0.4693\n",
      "Loss at step 114: 0.4692\n",
      "Loss at step 115: 0.4691\n",
      "Loss at step 116: 0.4691\n",
      "Loss at step 117: 0.4690\n",
      "Loss at step 118: 0.4689\n",
      "Loss at step 119: 0.4689\n",
      "Loss at step 120: 0.4688\n",
      "Loss at step 121: 0.4687\n",
      "Loss at step 122: 0.4687\n",
      "Loss at step 123: 0.4686\n",
      "Loss at step 124: 0.4686\n",
      "Loss at step 125: 0.4685\n",
      "Loss at step 126: 0.4684\n",
      "Loss at step 127: 0.4684\n",
      "Loss at step 128: 0.4683\n",
      "Loss at step 129: 0.4683\n",
      "Loss at step 130: 0.4682\n",
      "Loss at step 131: 0.4682\n",
      "Loss at step 132: 0.4681\n",
      "Loss at step 133: 0.4680\n",
      "Loss at step 134: 0.4680\n",
      "Loss at step 135: 0.4679\n",
      "Loss at step 136: 0.4679\n",
      "Loss at step 137: 0.4678\n",
      "Loss at step 138: 0.4678\n",
      "Loss at step 139: 0.4677\n",
      "Loss at step 140: 0.4677\n",
      "Loss at step 141: 0.4677\n",
      "Loss at step 142: 0.4676\n",
      "Loss at step 143: 0.4676\n",
      "Loss at step 144: 0.4675\n",
      "Loss at step 145: 0.4675\n",
      "Loss at step 146: 0.4674\n",
      "Loss at step 147: 0.4674\n",
      "Loss at step 148: 0.4673\n",
      "Loss at step 149: 0.4673\n",
      "Loss at step 150: 0.4673\n",
      "Loss at step 151: 0.4672\n",
      "Loss at step 152: 0.4672\n",
      "Loss at step 153: 0.4671\n",
      "Loss at step 154: 0.4671\n",
      "Loss at step 155: 0.4671\n",
      "Loss at step 156: 0.4670\n",
      "Loss at step 157: 0.4670\n",
      "Loss at step 158: 0.4670\n",
      "Loss at step 159: 0.4669\n",
      "Loss at step 160: 0.4669\n",
      "Loss at step 161: 0.4668\n",
      "Loss at step 162: 0.4668\n",
      "Loss at step 163: 0.4668\n",
      "Loss at step 164: 0.4667\n",
      "Loss at step 165: 0.4667\n",
      "Loss at step 166: 0.4667\n",
      "Loss at step 167: 0.4666\n",
      "Loss at step 168: 0.4666\n",
      "Loss at step 169: 0.4666\n",
      "Loss at step 170: 0.4666\n",
      "Loss at step 171: 0.4665\n",
      "Loss at step 172: 0.4665\n",
      "Loss at step 173: 0.4665\n",
      "Loss at step 174: 0.4664\n",
      "Loss at step 175: 0.4664\n",
      "Loss at step 176: 0.4664\n",
      "Loss at step 177: 0.4663\n",
      "Loss at step 178: 0.4663\n",
      "Loss at step 179: 0.4663\n",
      "Loss at step 180: 0.4663\n",
      "Loss at step 181: 0.4662\n",
      "Loss at step 182: 0.4662\n",
      "Loss at step 183: 0.4662\n",
      "Loss at step 184: 0.4662\n",
      "Loss at step 185: 0.4661\n",
      "Loss at step 186: 0.4661\n",
      "Loss at step 187: 0.4661\n",
      "Loss at step 188: 0.4661\n",
      "Loss at step 189: 0.4660\n",
      "Loss at step 190: 0.4660\n",
      "Loss at step 191: 0.4660\n",
      "Loss at step 192: 0.4660\n",
      "Loss at step 193: 0.4659\n",
      "Loss at step 194: 0.4659\n",
      "Loss at step 195: 0.4659\n",
      "Loss at step 196: 0.4659\n",
      "Loss at step 197: 0.4658\n",
      "Loss at step 198: 0.4658\n",
      "Loss at step 199: 0.4658\n",
      "Loss at step 200: 0.4658\n",
      "Loss at step 201: 0.4658\n",
      "Loss at step 202: 0.4657\n",
      "Loss at step 203: 0.4657\n",
      "Loss at step 204: 0.4657\n",
      "Loss at step 205: 0.4657\n",
      "Loss at step 206: 0.4657\n",
      "Loss at step 207: 0.4656\n",
      "Loss at step 208: 0.4656\n",
      "Loss at step 209: 0.4656\n",
      "Loss at step 210: 0.4656\n",
      "Loss at step 211: 0.4656\n",
      "Loss at step 212: 0.4655\n",
      "Loss at step 213: 0.4655\n",
      "Loss at step 214: 0.4655\n",
      "Loss at step 215: 0.4655\n",
      "Loss at step 216: 0.4655\n",
      "Loss at step 217: 0.4655\n",
      "Loss at step 218: 0.4654\n",
      "Loss at step 219: 0.4654\n",
      "Loss at step 220: 0.4654\n",
      "Loss at step 221: 0.4654\n",
      "Loss at step 222: 0.4654\n",
      "Loss at step 223: 0.4654\n",
      "Loss at step 224: 0.4653\n",
      "Loss at step 225: 0.4653\n",
      "Loss at step 226: 0.4653\n",
      "Loss at step 227: 0.4653\n",
      "Loss at step 228: 0.4653\n",
      "Loss at step 229: 0.4653\n",
      "Loss at step 230: 0.4652\n",
      "Loss at step 231: 0.4652\n",
      "Loss at step 232: 0.4652\n",
      "Loss at step 233: 0.4652\n",
      "Loss at step 234: 0.4652\n",
      "Loss at step 235: 0.4652\n",
      "Loss at step 236: 0.4652\n",
      "Loss at step 237: 0.4651\n",
      "Loss at step 238: 0.4651\n",
      "Loss at step 239: 0.4651\n",
      "Loss at step 240: 0.4651\n",
      "Loss at step 241: 0.4651\n",
      "Loss at step 242: 0.4651\n",
      "Loss at step 243: 0.4651\n",
      "Loss at step 244: 0.4650\n",
      "Loss at step 245: 0.4650\n",
      "Loss at step 246: 0.4650\n",
      "Loss at step 247: 0.4650\n",
      "Loss at step 248: 0.4650\n",
      "Loss at step 249: 0.4650\n",
      "Loss at step 250: 0.4650\n",
      "Loss at step 251: 0.4650\n",
      "Loss at step 252: 0.4649\n",
      "Loss at step 253: 0.4649\n",
      "Loss at step 254: 0.4649\n",
      "Loss at step 255: 0.4649\n",
      "Loss at step 256: 0.4649\n",
      "Loss at step 257: 0.4649\n",
      "Loss at step 258: 0.4649\n",
      "Loss at step 259: 0.4649\n",
      "Loss at step 260: 0.4649\n",
      "Loss at step 261: 0.4648\n",
      "Loss at step 262: 0.4648\n",
      "Loss at step 263: 0.4648\n",
      "Loss at step 264: 0.4648\n",
      "Loss at step 265: 0.4648\n",
      "Loss at step 266: 0.4648\n",
      "Loss at step 267: 0.4648\n",
      "Loss at step 268: 0.4648\n",
      "Loss at step 269: 0.4648\n",
      "Loss at step 270: 0.4648\n",
      "Loss at step 271: 0.4647\n",
      "Loss at step 272: 0.4647\n",
      "Loss at step 273: 0.4647\n",
      "Loss at step 274: 0.4647\n",
      "Loss at step 275: 0.4647\n",
      "Loss at step 276: 0.4647\n",
      "Loss at step 277: 0.4647\n",
      "Loss at step 278: 0.4647\n",
      "Loss at step 279: 0.4647\n",
      "Loss at step 280: 0.4647\n",
      "Loss at step 281: 0.4646\n",
      "Loss at step 282: 0.4646\n",
      "Loss at step 283: 0.4646\n",
      "Loss at step 284: 0.4646\n",
      "Loss at step 285: 0.4646\n",
      "Loss at step 286: 0.4646\n",
      "Loss at step 287: 0.4646\n",
      "Loss at step 288: 0.4646\n",
      "Loss at step 289: 0.4646\n",
      "Loss at step 290: 0.4646\n",
      "Loss at step 291: 0.4646\n",
      "Loss at step 292: 0.4646\n",
      "Loss at step 293: 0.4645\n",
      "Loss at step 294: 0.4645\n",
      "Loss at step 295: 0.4645\n",
      "Loss at step 296: 0.4645\n",
      "Loss at step 297: 0.4645\n",
      "Loss at step 298: 0.4645\n",
      "Loss at step 299: 0.4645\n",
      "Loss at step 300: 0.4645\n",
      "Loss at step 301: 0.4645\n",
      "Loss at step 302: 0.4645\n",
      "Loss at step 303: 0.4645\n",
      "Loss at step 304: 0.4645\n",
      "Loss at step 305: 0.4645\n",
      "Loss at step 306: 0.4644\n",
      "Loss at step 307: 0.4644\n",
      "Loss at step 308: 0.4644\n",
      "Loss at step 309: 0.4644\n",
      "Loss at step 310: 0.4644\n",
      "Loss at step 311: 0.4644\n",
      "Loss at step 312: 0.4644\n",
      "Loss at step 313: 0.4644\n",
      "Loss at step 314: 0.4644\n",
      "Loss at step 315: 0.4644\n",
      "Loss at step 316: 0.4644\n",
      "Loss at step 317: 0.4644\n",
      "Loss at step 318: 0.4644\n",
      "Loss at step 319: 0.4644\n",
      "Loss at step 320: 0.4644\n",
      "Loss at step 321: 0.4643\n",
      "Loss at step 322: 0.4643\n",
      "Loss at step 323: 0.4643\n",
      "Loss at step 324: 0.4643\n",
      "Loss at step 325: 0.4643\n",
      "Loss at step 326: 0.4643\n",
      "Loss at step 327: 0.4643\n",
      "Loss at step 328: 0.4643\n",
      "Loss at step 329: 0.4643\n",
      "Loss at step 330: 0.4643\n",
      "Loss at step 331: 0.4643\n",
      "Loss at step 332: 0.4643\n",
      "Loss at step 333: 0.4643\n",
      "Loss at step 334: 0.4643\n",
      "Loss at step 335: 0.4643\n",
      "Loss at step 336: 0.4643\n",
      "Loss at step 337: 0.4643\n",
      "Loss at step 338: 0.4643\n",
      "Loss at step 339: 0.4642\n",
      "Loss at step 340: 0.4642\n",
      "Loss at step 341: 0.4642\n",
      "Loss at step 342: 0.4642\n",
      "Loss at step 343: 0.4642\n",
      "Loss at step 344: 0.4642\n",
      "Loss at step 345: 0.4642\n",
      "Loss at step 346: 0.4642\n",
      "Loss at step 347: 0.4642\n",
      "Loss at step 348: 0.4642\n",
      "Loss at step 349: 0.4642\n",
      "Loss at step 350: 0.4642\n",
      "Loss at step 351: 0.4642\n",
      "Loss at step 352: 0.4642\n",
      "Loss at step 353: 0.4642\n",
      "Loss at step 354: 0.4642\n",
      "Loss at step 355: 0.4642\n",
      "Loss at step 356: 0.4642\n",
      "Loss at step 357: 0.4642\n",
      "Loss at step 358: 0.4642\n",
      "Loss at step 359: 0.4641\n",
      "Loss at step 360: 0.4641\n",
      "Loss at step 361: 0.4641\n",
      "Loss at step 362: 0.4641\n",
      "Loss at step 363: 0.4641\n",
      "Loss at step 364: 0.4641\n",
      "Loss at step 365: 0.4641\n",
      "Loss at step 366: 0.4641\n",
      "Loss at step 367: 0.4641\n",
      "Loss at step 368: 0.4641\n",
      "Loss at step 369: 0.4641\n",
      "Loss at step 370: 0.4641\n",
      "Loss at step 371: 0.4641\n",
      "Loss at step 372: 0.4641\n",
      "Loss at step 373: 0.4641\n",
      "Loss at step 374: 0.4641\n",
      "Loss at step 375: 0.4641\n",
      "Loss at step 376: 0.4641\n",
      "Loss at step 377: 0.4641\n",
      "Loss at step 378: 0.4641\n",
      "Loss at step 379: 0.4641\n",
      "Loss at step 380: 0.4641\n",
      "Loss at step 381: 0.4641\n",
      "Loss at step 382: 0.4641\n",
      "Loss at step 383: 0.4641\n",
      "Loss at step 384: 0.4640\n",
      "Loss at step 385: 0.4640\n",
      "Loss at step 386: 0.4640\n",
      "Loss at step 387: 0.4640\n",
      "Loss at step 388: 0.4640\n",
      "Loss at step 389: 0.4640\n",
      "Loss at step 390: 0.4640\n",
      "Loss at step 391: 0.4640\n",
      "Loss at step 392: 0.4640\n",
      "Loss at step 393: 0.4640\n",
      "Loss at step 394: 0.4640\n",
      "Loss at step 395: 0.4640\n",
      "Loss at step 396: 0.4640\n",
      "Loss at step 397: 0.4640\n",
      "Loss at step 398: 0.4640\n",
      "Loss at step 399: 0.4640\n",
      "Loss at step 400: 0.4640\n",
      "Loss at step 401: 0.4640\n",
      "Loss at step 402: 0.4640\n",
      "Loss at step 403: 0.4640\n",
      "Loss at step 404: 0.4640\n",
      "Loss at step 405: 0.4640\n",
      "Loss at step 406: 0.4640\n",
      "Loss at step 407: 0.4640\n",
      "Loss at step 408: 0.4640\n",
      "Loss at step 409: 0.4640\n",
      "Loss at step 410: 0.4640\n",
      "Loss at step 411: 0.4640\n",
      "Loss at step 412: 0.4640\n",
      "Loss at step 413: 0.4640\n",
      "Loss at step 414: 0.4639\n",
      "Loss at step 415: 0.4639\n",
      "Loss at step 416: 0.4639\n",
      "Loss at step 417: 0.4639\n",
      "Loss at step 418: 0.4639\n",
      "Loss at step 419: 0.4639\n",
      "Loss at step 420: 0.4639\n",
      "Loss at step 421: 0.4639\n",
      "Loss at step 422: 0.4639\n",
      "Loss at step 423: 0.4639\n",
      "Loss at step 424: 0.4639\n",
      "Loss at step 425: 0.4639\n",
      "Loss at step 426: 0.4639\n",
      "Loss at step 427: 0.4639\n",
      "Loss at step 428: 0.4639\n",
      "Loss at step 429: 0.4639\n",
      "Loss at step 430: 0.4639\n",
      "Loss at step 431: 0.4639\n",
      "Loss at step 432: 0.4639\n",
      "Loss at step 433: 0.4639\n",
      "Loss at step 434: 0.4639\n",
      "Loss at step 435: 0.4639\n",
      "Loss at step 436: 0.4639\n",
      "Loss at step 437: 0.4639\n",
      "Loss at step 438: 0.4639\n",
      "Loss at step 439: 0.4639\n",
      "Loss at step 440: 0.4639\n",
      "Loss at step 441: 0.4639\n",
      "Loss at step 442: 0.4639\n",
      "Loss at step 443: 0.4639\n",
      "Loss at step 444: 0.4639\n",
      "Loss at step 445: 0.4639\n",
      "Loss at step 446: 0.4639\n",
      "Loss at step 447: 0.4639\n",
      "Loss at step 448: 0.4639\n",
      "Loss at step 449: 0.4639\n",
      "Loss at step 450: 0.4639\n",
      "Loss at step 451: 0.4639\n",
      "Loss at step 452: 0.4638\n",
      "Loss at step 453: 0.4638\n",
      "Loss at step 454: 0.4638\n",
      "Loss at step 455: 0.4638\n",
      "Loss at step 456: 0.4638\n",
      "Loss at step 457: 0.4638\n",
      "Loss at step 458: 0.4638\n",
      "Loss at step 459: 0.4638\n",
      "Loss at step 460: 0.4638\n",
      "Loss at step 461: 0.4638\n",
      "Loss at step 462: 0.4638\n",
      "Loss at step 463: 0.4638\n",
      "Loss at step 464: 0.4638\n",
      "Loss at step 465: 0.4638\n",
      "Loss at step 466: 0.4638\n",
      "Loss at step 467: 0.4638\n",
      "Loss at step 468: 0.4638\n",
      "Loss at step 469: 0.4638\n",
      "Loss at step 470: 0.4638\n",
      "Loss at step 471: 0.4638\n",
      "Loss at step 472: 0.4638\n",
      "Loss at step 473: 0.4638\n",
      "Loss at step 474: 0.4638\n",
      "Loss at step 475: 0.4638\n",
      "Loss at step 476: 0.4638\n",
      "Loss at step 477: 0.4638\n",
      "Loss at step 478: 0.4638\n",
      "Loss at step 479: 0.4638\n",
      "Loss at step 480: 0.4638\n",
      "Loss at step 481: 0.4638\n",
      "Loss at step 482: 0.4638\n",
      "Loss at step 483: 0.4638\n",
      "Loss at step 484: 0.4638\n",
      "Loss at step 485: 0.4638\n",
      "Loss at step 486: 0.4638\n",
      "Loss at step 487: 0.4638\n",
      "Loss at step 488: 0.4638\n",
      "Loss at step 489: 0.4638\n",
      "Loss at step 490: 0.4638\n",
      "Loss at step 491: 0.4638\n",
      "Loss at step 492: 0.4638\n",
      "Loss at step 493: 0.4638\n",
      "Loss at step 494: 0.4638\n",
      "Loss at step 495: 0.4638\n",
      "Loss at step 496: 0.4638\n",
      "Loss at step 497: 0.4638\n",
      "Loss at step 498: 0.4638\n",
      "Loss at step 499: 0.4638\n",
      "Loss at step 500: 0.4638\n",
      "Loss at step 501: 0.4638\n",
      "Loss at step 502: 0.4638\n",
      "Loss at step 503: 0.4638\n",
      "Loss at step 504: 0.4638\n",
      "Loss at step 505: 0.4638\n",
      "Loss at step 506: 0.4637\n",
      "Loss at step 507: 0.4637\n",
      "Loss at step 508: 0.4637\n",
      "Loss at step 509: 0.4637\n",
      "Loss at step 510: 0.4637\n",
      "Loss at step 511: 0.4637\n",
      "Loss at step 512: 0.4637\n",
      "Loss at step 513: 0.4637\n",
      "Loss at step 514: 0.4637\n",
      "Loss at step 515: 0.4637\n",
      "Loss at step 516: 0.4637\n",
      "Loss at step 517: 0.4637\n",
      "Loss at step 518: 0.4637\n",
      "Loss at step 519: 0.4637\n",
      "Loss at step 520: 0.4637\n",
      "Loss at step 521: 0.4637\n",
      "Loss at step 522: 0.4637\n",
      "Loss at step 523: 0.4637\n",
      "Loss at step 524: 0.4637\n",
      "Loss at step 525: 0.4637\n",
      "Loss at step 526: 0.4637\n",
      "Loss at step 527: 0.4637\n",
      "Loss at step 528: 0.4637\n",
      "Loss at step 529: 0.4637\n",
      "Loss at step 530: 0.4637\n",
      "Loss at step 531: 0.4637\n",
      "Loss at step 532: 0.4637\n",
      "Loss at step 533: 0.4637\n",
      "Loss at step 534: 0.4637\n",
      "Loss at step 535: 0.4637\n",
      "Loss at step 536: 0.4637\n",
      "Loss at step 537: 0.4637\n",
      "Loss at step 538: 0.4637\n",
      "Loss at step 539: 0.4637\n",
      "Loss at step 540: 0.4637\n",
      "Loss at step 541: 0.4637\n",
      "Loss at step 542: 0.4637\n",
      "Loss at step 543: 0.4637\n",
      "Loss at step 544: 0.4637\n",
      "Loss at step 545: 0.4637\n",
      "Loss at step 546: 0.4637\n",
      "Loss at step 547: 0.4637\n",
      "Loss at step 548: 0.4637\n",
      "Loss at step 549: 0.4637\n",
      "Loss at step 550: 0.4637\n",
      "Loss at step 551: 0.4637\n",
      "Loss at step 552: 0.4637\n",
      "Loss at step 553: 0.4637\n",
      "Loss at step 554: 0.4637\n",
      "Loss at step 555: 0.4637\n",
      "Loss at step 556: 0.4637\n",
      "Loss at step 557: 0.4637\n",
      "Loss at step 558: 0.4637\n",
      "Loss at step 559: 0.4637\n",
      "Loss at step 560: 0.4637\n",
      "Loss at step 561: 0.4637\n",
      "Loss at step 562: 0.4637\n",
      "Loss at step 563: 0.4637\n",
      "Loss at step 564: 0.4637\n",
      "Loss at step 565: 0.4637\n",
      "Loss at step 566: 0.4637\n",
      "Loss at step 567: 0.4637\n",
      "Loss at step 568: 0.4637\n",
      "Loss at step 569: 0.4637\n",
      "Loss at step 570: 0.4637\n",
      "Loss at step 571: 0.4637\n",
      "Loss at step 572: 0.4637\n",
      "Loss at step 573: 0.4637\n",
      "Loss at step 574: 0.4637\n",
      "Loss at step 575: 0.4637\n",
      "Loss at step 576: 0.4637\n",
      "Loss at step 577: 0.4637\n",
      "Loss at step 578: 0.4637\n",
      "Loss at step 579: 0.4637\n",
      "Loss at step 580: 0.4637\n",
      "Loss at step 581: 0.4637\n",
      "Loss at step 582: 0.4637\n",
      "Loss at step 583: 0.4637\n",
      "Loss at step 584: 0.4637\n",
      "Loss at step 585: 0.4637\n",
      "Loss at step 586: 0.4637\n",
      "Loss at step 587: 0.4636\n",
      "Loss at step 588: 0.4636\n",
      "Loss at step 589: 0.4636\n",
      "Loss at step 590: 0.4636\n",
      "Loss at step 591: 0.4636\n",
      "Loss at step 592: 0.4636\n",
      "Loss at step 593: 0.4636\n",
      "Loss at step 594: 0.4636\n",
      "Loss at step 595: 0.4636\n",
      "Loss at step 596: 0.4636\n",
      "Loss at step 597: 0.4636\n",
      "Loss at step 598: 0.4636\n",
      "Loss at step 599: 0.4636\n",
      "Loss at step 600: 0.4636\n",
      "Loss at step 601: 0.4636\n",
      "Loss at step 602: 0.4636\n",
      "Loss at step 603: 0.4636\n",
      "Loss at step 604: 0.4636\n",
      "Loss at step 605: 0.4636\n",
      "Loss at step 606: 0.4636\n",
      "Loss at step 607: 0.4636\n",
      "Loss at step 608: 0.4636\n",
      "Loss at step 609: 0.4636\n",
      "Loss at step 610: 0.4636\n",
      "Loss at step 611: 0.4636\n",
      "Loss at step 612: 0.4636\n",
      "Loss at step 613: 0.4636\n",
      "Loss at step 614: 0.4636\n",
      "Loss at step 615: 0.4636\n",
      "Loss at step 616: 0.4636\n",
      "Loss at step 617: 0.4636\n",
      "Loss at step 618: 0.4636\n",
      "Loss at step 619: 0.4636\n",
      "Loss at step 620: 0.4636\n",
      "Loss at step 621: 0.4636\n",
      "Loss at step 622: 0.4636\n",
      "Loss at step 623: 0.4636\n",
      "Loss at step 624: 0.4636\n",
      "Loss at step 625: 0.4636\n",
      "Loss at step 626: 0.4636\n",
      "Loss at step 627: 0.4636\n",
      "Loss at step 628: 0.4636\n",
      "Loss at step 629: 0.4636\n",
      "Loss at step 630: 0.4636\n",
      "Loss at step 631: 0.4636\n",
      "Loss at step 632: 0.4636\n",
      "Loss at step 633: 0.4636\n",
      "Loss at step 634: 0.4636\n",
      "Loss at step 635: 0.4636\n",
      "Loss at step 636: 0.4636\n",
      "Loss at step 637: 0.4636\n",
      "Loss at step 638: 0.4636\n",
      "Loss at step 639: 0.4636\n",
      "Loss at step 640: 0.4636\n",
      "Loss at step 641: 0.4636\n",
      "Loss at step 642: 0.4636\n",
      "Loss at step 643: 0.4636\n",
      "Loss at step 644: 0.4636\n",
      "Loss at step 645: 0.4636\n",
      "Loss at step 646: 0.4636\n",
      "Loss at step 647: 0.4636\n",
      "Loss at step 648: 0.4636\n",
      "Loss at step 649: 0.4636\n",
      "Loss at step 650: 0.4636\n",
      "Loss at step 651: 0.4636\n",
      "Loss at step 652: 0.4636\n",
      "Loss at step 653: 0.4636\n",
      "Loss at step 654: 0.4636\n",
      "Loss at step 655: 0.4636\n",
      "Loss at step 656: 0.4636\n",
      "Loss at step 657: 0.4636\n",
      "Loss at step 658: 0.4636\n",
      "Loss at step 659: 0.4636\n",
      "Loss at step 660: 0.4636\n",
      "Loss at step 661: 0.4636\n",
      "Loss at step 662: 0.4636\n",
      "Loss at step 663: 0.4636\n",
      "Loss at step 664: 0.4636\n",
      "Loss at step 665: 0.4636\n",
      "Loss at step 666: 0.4636\n",
      "Loss at step 667: 0.4636\n",
      "Loss at step 668: 0.4636\n",
      "Loss at step 669: 0.4636\n",
      "Loss at step 670: 0.4636\n",
      "Loss at step 671: 0.4636\n",
      "Loss at step 672: 0.4636\n",
      "Loss at step 673: 0.4636\n",
      "Loss at step 674: 0.4636\n",
      "Loss at step 675: 0.4636\n",
      "Loss at step 676: 0.4636\n",
      "Loss at step 677: 0.4636\n",
      "Loss at step 678: 0.4636\n",
      "Loss at step 679: 0.4636\n",
      "Loss at step 680: 0.4636\n",
      "Loss at step 681: 0.4636\n",
      "Loss at step 682: 0.4636\n",
      "Loss at step 683: 0.4636\n",
      "Loss at step 684: 0.4636\n",
      "Loss at step 685: 0.4636\n",
      "Loss at step 686: 0.4636\n",
      "Loss at step 687: 0.4636\n",
      "Loss at step 688: 0.4636\n",
      "Loss at step 689: 0.4636\n",
      "Loss at step 690: 0.4636\n",
      "Loss at step 691: 0.4636\n",
      "Loss at step 692: 0.4636\n",
      "Loss at step 693: 0.4636\n",
      "Loss at step 694: 0.4636\n",
      "Loss at step 695: 0.4636\n",
      "Loss at step 696: 0.4636\n",
      "Loss at step 697: 0.4636\n",
      "Loss at step 698: 0.4636\n",
      "Loss at step 699: 0.4636\n",
      "Loss at step 700: 0.4636\n",
      "Loss at step 701: 0.4636\n",
      "Loss at step 702: 0.4636\n",
      "Loss at step 703: 0.4636\n",
      "Loss at step 704: 0.4636\n",
      "Loss at step 705: 0.4636\n",
      "Loss at step 706: 0.4636\n",
      "Loss at step 707: 0.4636\n",
      "Loss at step 708: 0.4636\n",
      "Loss at step 709: 0.4636\n",
      "Loss at step 710: 0.4636\n",
      "Loss at step 711: 0.4636\n",
      "Loss at step 712: 0.4636\n",
      "Loss at step 713: 0.4636\n",
      "Loss at step 714: 0.4636\n",
      "Loss at step 715: 0.4636\n",
      "Loss at step 716: 0.4636\n",
      "Loss at step 717: 0.4636\n",
      "Loss at step 718: 0.4636\n",
      "Loss at step 719: 0.4636\n",
      "Loss at step 720: 0.4636\n",
      "Loss at step 721: 0.4636\n",
      "Loss at step 722: 0.4636\n",
      "Loss at step 723: 0.4636\n",
      "Loss at step 724: 0.4636\n",
      "Loss at step 725: 0.4636\n",
      "Loss at step 726: 0.4636\n",
      "Loss at step 727: 0.4636\n",
      "Loss at step 728: 0.4636\n",
      "Loss at step 729: 0.4636\n",
      "Loss at step 730: 0.4636\n",
      "Loss at step 731: 0.4636\n",
      "Loss at step 732: 0.4636\n",
      "Loss at step 733: 0.4636\n",
      "Loss at step 734: 0.4636\n",
      "Loss at step 735: 0.4636\n",
      "Loss at step 736: 0.4636\n",
      "Loss at step 737: 0.4636\n",
      "Loss at step 738: 0.4636\n",
      "Loss at step 739: 0.4636\n",
      "Loss at step 740: 0.4636\n",
      "Loss at step 741: 0.4636\n",
      "Loss at step 742: 0.4636\n",
      "Loss at step 743: 0.4636\n",
      "Loss at step 744: 0.4636\n",
      "Loss at step 745: 0.4636\n",
      "Loss at step 746: 0.4636\n",
      "Loss at step 747: 0.4636\n",
      "Loss at step 748: 0.4636\n",
      "Loss at step 749: 0.4635\n",
      "Loss at step 750: 0.4635\n",
      "Loss at step 751: 0.4635\n",
      "Loss at step 752: 0.4635\n",
      "Loss at step 753: 0.4635\n",
      "Loss at step 754: 0.4635\n",
      "Loss at step 755: 0.4635\n",
      "Loss at step 756: 0.4635\n",
      "Loss at step 757: 0.4635\n",
      "Loss at step 758: 0.4635\n",
      "Loss at step 759: 0.4635\n",
      "Loss at step 760: 0.4635\n",
      "Loss at step 761: 0.4635\n",
      "Loss at step 762: 0.4635\n",
      "Loss at step 763: 0.4635\n",
      "Loss at step 764: 0.4635\n",
      "Loss at step 765: 0.4635\n",
      "Loss at step 766: 0.4635\n",
      "Loss at step 767: 0.4635\n",
      "Loss at step 768: 0.4635\n",
      "Loss at step 769: 0.4635\n",
      "Loss at step 770: 0.4635\n",
      "Loss at step 771: 0.4635\n",
      "Loss at step 772: 0.4635\n",
      "Loss at step 773: 0.4635\n",
      "Loss at step 774: 0.4635\n",
      "Loss at step 775: 0.4635\n",
      "Loss at step 776: 0.4635\n",
      "Loss at step 777: 0.4635\n",
      "Loss at step 778: 0.4635\n",
      "Loss at step 779: 0.4635\n",
      "Loss at step 780: 0.4635\n",
      "Loss at step 781: 0.4635\n",
      "Loss at step 782: 0.4635\n",
      "Loss at step 783: 0.4635\n",
      "Loss at step 784: 0.4635\n",
      "Loss at step 785: 0.4635\n",
      "Loss at step 786: 0.4635\n",
      "Loss at step 787: 0.4635\n",
      "Loss at step 788: 0.4635\n",
      "Loss at step 789: 0.4635\n",
      "Loss at step 790: 0.4635\n",
      "Loss at step 791: 0.4635\n",
      "Loss at step 792: 0.4635\n",
      "Loss at step 793: 0.4635\n",
      "Loss at step 794: 0.4635\n",
      "Loss at step 795: 0.4635\n",
      "Loss at step 796: 0.4635\n",
      "Loss at step 797: 0.4635\n",
      "Loss at step 798: 0.4635\n",
      "Loss at step 799: 0.4635\n",
      "Loss at step 800: 0.4635\n",
      "Loss at step 801: 0.4635\n",
      "Loss at step 802: 0.4635\n",
      "Loss at step 803: 0.4635\n",
      "Loss at step 804: 0.4635\n",
      "Loss at step 805: 0.4635\n",
      "Loss at step 806: 0.4635\n",
      "Loss at step 807: 0.4635\n",
      "Loss at step 808: 0.4635\n",
      "Loss at step 809: 0.4635\n",
      "Loss at step 810: 0.4635\n",
      "Loss at step 811: 0.4635\n",
      "Loss at step 812: 0.4635\n",
      "Loss at step 813: 0.4635\n",
      "Loss at step 814: 0.4635\n",
      "Loss at step 815: 0.4635\n",
      "Loss at step 816: 0.4635\n",
      "Loss at step 817: 0.4635\n",
      "Loss at step 818: 0.4635\n",
      "Loss at step 819: 0.4635\n",
      "Loss at step 820: 0.4635\n",
      "Loss at step 821: 0.4635\n",
      "Loss at step 822: 0.4635\n",
      "Loss at step 823: 0.4635\n",
      "Loss at step 824: 0.4635\n",
      "Loss at step 825: 0.4635\n",
      "Loss at step 826: 0.4635\n",
      "Loss at step 827: 0.4635\n",
      "Loss at step 828: 0.4635\n",
      "Loss at step 829: 0.4635\n",
      "Loss at step 830: 0.4635\n",
      "Loss at step 831: 0.4635\n",
      "Loss at step 832: 0.4635\n",
      "Loss at step 833: 0.4635\n",
      "Loss at step 834: 0.4635\n",
      "Loss at step 835: 0.4635\n",
      "Loss at step 836: 0.4635\n",
      "Loss at step 837: 0.4635\n",
      "Loss at step 838: 0.4635\n",
      "Loss at step 839: 0.4635\n",
      "Loss at step 840: 0.4635\n",
      "Loss at step 841: 0.4635\n",
      "Loss at step 842: 0.4635\n",
      "Loss at step 843: 0.4635\n",
      "Loss at step 844: 0.4635\n",
      "Loss at step 845: 0.4635\n",
      "Loss at step 846: 0.4635\n",
      "Loss at step 847: 0.4635\n",
      "Loss at step 848: 0.4635\n",
      "Loss at step 849: 0.4635\n",
      "Loss at step 850: 0.4635\n",
      "Loss at step 851: 0.4635\n",
      "Loss at step 852: 0.4635\n",
      "Loss at step 853: 0.4635\n",
      "Loss at step 854: 0.4635\n",
      "Loss at step 855: 0.4635\n",
      "Loss at step 856: 0.4635\n",
      "Loss at step 857: 0.4635\n",
      "Loss at step 858: 0.4635\n",
      "Loss at step 859: 0.4635\n",
      "Loss at step 860: 0.4635\n",
      "Loss at step 861: 0.4635\n",
      "Loss at step 862: 0.4635\n",
      "Loss at step 863: 0.4635\n",
      "Loss at step 864: 0.4635\n",
      "Loss at step 865: 0.4635\n",
      "Loss at step 866: 0.4635\n",
      "Loss at step 867: 0.4635\n",
      "Loss at step 868: 0.4635\n",
      "Loss at step 869: 0.4635\n",
      "Loss at step 870: 0.4635\n",
      "Loss at step 871: 0.4635\n",
      "Loss at step 872: 0.4635\n",
      "Loss at step 873: 0.4635\n",
      "Loss at step 874: 0.4635\n",
      "Loss at step 875: 0.4635\n",
      "Loss at step 876: 0.4635\n",
      "Loss at step 877: 0.4635\n",
      "Loss at step 878: 0.4635\n",
      "Loss at step 879: 0.4635\n",
      "Loss at step 880: 0.4635\n",
      "Loss at step 881: 0.4635\n",
      "Loss at step 882: 0.4635\n",
      "Loss at step 883: 0.4635\n",
      "Loss at step 884: 0.4635\n",
      "Loss at step 885: 0.4635\n",
      "Loss at step 886: 0.4635\n",
      "Loss at step 887: 0.4635\n",
      "Loss at step 888: 0.4635\n",
      "Loss at step 889: 0.4635\n",
      "Loss at step 890: 0.4635\n",
      "Loss at step 891: 0.4635\n",
      "Loss at step 892: 0.4635\n",
      "Loss at step 893: 0.4635\n",
      "Loss at step 894: 0.4635\n",
      "Loss at step 895: 0.4635\n",
      "Loss at step 896: 0.4635\n",
      "Loss at step 897: 0.4635\n",
      "Loss at step 898: 0.4635\n",
      "Loss at step 899: 0.4635\n",
      "Loss at step 900: 0.4635\n",
      "Loss at step 901: 0.4635\n",
      "Loss at step 902: 0.4635\n",
      "Loss at step 903: 0.4635\n",
      "Loss at step 904: 0.4635\n",
      "Loss at step 905: 0.4635\n",
      "Loss at step 906: 0.4635\n",
      "Loss at step 907: 0.4635\n",
      "Loss at step 908: 0.4635\n",
      "Loss at step 909: 0.4635\n",
      "Loss at step 910: 0.4635\n",
      "Loss at step 911: 0.4635\n",
      "Loss at step 912: 0.4635\n",
      "Loss at step 913: 0.4635\n",
      "Loss at step 914: 0.4635\n",
      "Loss at step 915: 0.4635\n",
      "Loss at step 916: 0.4635\n",
      "Loss at step 917: 0.4635\n",
      "Loss at step 918: 0.4635\n",
      "Loss at step 919: 0.4635\n",
      "Loss at step 920: 0.4635\n",
      "Loss at step 921: 0.4635\n",
      "Loss at step 922: 0.4635\n",
      "Loss at step 923: 0.4635\n",
      "Loss at step 924: 0.4635\n",
      "Loss at step 925: 0.4635\n",
      "Loss at step 926: 0.4635\n",
      "Loss at step 927: 0.4635\n",
      "Loss at step 928: 0.4635\n",
      "Loss at step 929: 0.4635\n",
      "Loss at step 930: 0.4635\n",
      "Loss at step 931: 0.4635\n",
      "Loss at step 932: 0.4635\n",
      "Loss at step 933: 0.4635\n",
      "Loss at step 934: 0.4635\n",
      "Loss at step 935: 0.4635\n",
      "Loss at step 936: 0.4635\n",
      "Loss at step 937: 0.4635\n",
      "Loss at step 938: 0.4635\n",
      "Loss at step 939: 0.4635\n",
      "Loss at step 940: 0.4635\n",
      "Loss at step 941: 0.4635\n",
      "Loss at step 942: 0.4635\n",
      "Loss at step 943: 0.4635\n",
      "Loss at step 944: 0.4635\n",
      "Loss at step 945: 0.4635\n",
      "Loss at step 946: 0.4635\n",
      "Loss at step 947: 0.4635\n",
      "Loss at step 948: 0.4635\n",
      "Loss at step 949: 0.4635\n",
      "Loss at step 950: 0.4635\n",
      "Loss at step 951: 0.4635\n",
      "Loss at step 952: 0.4635\n",
      "Loss at step 953: 0.4635\n",
      "Loss at step 954: 0.4635\n",
      "Loss at step 955: 0.4635\n",
      "Loss at step 956: 0.4635\n",
      "Loss at step 957: 0.4635\n",
      "Loss at step 958: 0.4635\n",
      "Loss at step 959: 0.4635\n",
      "Loss at step 960: 0.4635\n",
      "Loss at step 961: 0.4635\n",
      "Loss at step 962: 0.4635\n",
      "Loss at step 963: 0.4635\n",
      "Loss at step 964: 0.4635\n",
      "Loss at step 965: 0.4635\n",
      "Loss at step 966: 0.4635\n",
      "Loss at step 967: 0.4635\n",
      "Loss at step 968: 0.4635\n",
      "Loss at step 969: 0.4635\n",
      "Loss at step 970: 0.4635\n",
      "Loss at step 971: 0.4635\n",
      "Loss at step 972: 0.4635\n",
      "Loss at step 973: 0.4635\n",
      "Loss at step 974: 0.4635\n",
      "Loss at step 975: 0.4635\n",
      "Loss at step 976: 0.4635\n",
      "Loss at step 977: 0.4635\n",
      "Loss at step 978: 0.4635\n",
      "Loss at step 979: 0.4635\n",
      "Loss at step 980: 0.4635\n",
      "Loss at step 981: 0.4635\n",
      "Loss at step 982: 0.4635\n",
      "Loss at step 983: 0.4635\n",
      "Loss at step 984: 0.4635\n",
      "Loss at step 985: 0.4635\n",
      "Loss at step 986: 0.4635\n",
      "Loss at step 987: 0.4635\n",
      "Loss at step 988: 0.4635\n",
      "Loss at step 989: 0.4635\n",
      "Loss at step 990: 0.4635\n",
      "Loss at step 991: 0.4635\n",
      "Loss at step 992: 0.4635\n",
      "Loss at step 993: 0.4635\n",
      "Loss at step 994: 0.4635\n",
      "Loss at step 995: 0.4635\n",
      "Loss at step 996: 0.4635\n",
      "Loss at step 997: 0.4635\n",
      "Loss at step 998: 0.4635\n",
      "Loss at step 999: 0.4635\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    loss = training_step(train_data, labble)\n",
    "    print(f\"Loss at step {step}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "f1ff2aad-b5e0-4c5e-9752-6a77d365c8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 1), dtype=float32, numpy=\n",
       "array([[1.        ],\n",
       "       [0.86252075],\n",
       "       [1.        ],\n",
       "       [0.86252075],\n",
       "       [1.        ],\n",
       "       [0.86252075],\n",
       "       [1.        ],\n",
       "       [0.86252075],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [0.86252075],\n",
       "       [1.        ],\n",
       "       [0.86252075],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [0.86252075],\n",
       "       [1.        ],\n",
       "       [1.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vc = tf.Variable(initial_value=v_c_x, dtype=tf.float32)\n",
    "vt = tf.Variable(initial_value=v_t_x, dtype=tf.float32)\n",
    "model(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650524d9-4910-4466-9d60-da7177ea674c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aigo",
   "language": "python",
   "name": "aigo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
